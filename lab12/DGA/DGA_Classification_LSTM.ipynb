{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DGA_Classification_LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Πρόσφατες επιθέσεις, όπως εκείνη στον πάροχο DNS Dyn (<a href=\"https://en.wikipedia.org/wiki/DDoS_attack_on_Dyn\">wiki</a>) αποδεικνύουν ότι ένα από τα σημαντικότερα προβλήματα που αντιμετωπίζει το σύγχρονο Διαδίκτυο είναι εκείνο των botnets. Σε αυτά, ένας επιτιθέμενος συγκεντρώνει την υπολογιστική ισχύ που του είναι απαραίτητη για την εκδήλωση επιθέσεων DDoS ή/και άλλων κακόβουλων δραστηριοτήτων εγκαθιστώντας λογισμικό σε μεγάλο πλήθος από υπολογιστές (bots) που έχουν κενά ασφαλείας (π.χ. συσκευές Internet of Things - IoT).\n",
        "\n",
        "Οι μολυσμένοι υπολογιστές (bots) διατηρούν διαύλους επικοινωνίας με το διαχειριστή του botnet (Command & Control Server) με σκοπό να λαμβάνουν εντολές και να αποστέλλουν πληροφορίες. Για το σκοπό αυτό εκμεταλλεύονται καθιερωμένα πρωτόκολλα, όπως το DNS με την παραγωγή μεγάλου πλήθους από domain names μέσω Domain Generation Algorithms (DGA's) που αλλάζουν διαρκώς για την επικοινωνία του bot με το διαχειριστή του, ώστε να αποφεύγεται ο εντοπισμός του Command & Control Server.\n",
        "\n",
        "Τα ονόματα DNS που χρησιμοποιούνται από αλγορίθμους DGA μπορεί να είναι είτε τυχαία αλφαριθμητικά (π.χ. asdfasjkdfh8oawher8has.com) ή συνδυασμοί τυχαίων λέξεων που έχουν ληφθεί από κάποιο λεξικό (π.χ. school-doctor.com). Χρησιμοποιείται ένας μεγάλος αριθμός από τέτοια ονόματα, η πλειοψηφία των οποίων δεν έχουν κάποια αντιστοίχιση σε διεύθυνση IP και στοχεύουν στην απόκρυψη του Command & Control Server, επειδή οι αμυνόμενοι καλούνται να ελέγξουν κάθε ένα από τα ονόματα που παρατηρούν στο δίκτυό τους, σπαταλώντας χρόνο και πόρους. Επιπρόσθετα, η διεύθυνση IP του Command & Control Server αλλάζει πολύ συχνά (πολλές φορές σε μία μέρα), ώστε να αποφεύγεται ο εντοπισμός του ακόμα και όταν εντοπίζονται τα ονόματα DGA που οδήγησαν σε αυτόν."
      ],
      "metadata": {
        "id": "RczVgvwwHX0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/nkostopoulos/StochasticsLabPublic/master/lab12/dga.png\"></img>"
      ],
      "metadata": {
        "id": "XaN28mnKLHlO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"\"></img>"
      ],
      "metadata": {
        "id": "xvWLJ4gvJdx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.recurrent import LSTM\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "def add_word_distinct_chars(string, distinct_chars):\n",
        "    for char in string:\n",
        "        distinct_chars.add(char)\n",
        "    return distinct_chars\n",
        "\n",
        "def find_max_len(string, max_len):\n",
        "    string_length = len(string)\n",
        "    if string_length > max_len:\n",
        "        max_len = string_length\n",
        "    return max_len\n",
        "\n",
        "def load_data(filename):\n",
        "    dataset = []\n",
        "    distinct_chars = set()\n",
        "    max_len = 0\n",
        "    currentIndex = 0\n",
        "    diverse_labels = dict()\n",
        "    with open(filename, \"r\") as fdr:\n",
        "        for line in fdr:\n",
        "            line = line.strip()\n",
        "            general, label, name = line.split(\",\")\n",
        "            name = name.split(\".\")[0]\n",
        "            if label not in diverse_labels.keys():\n",
        "                diverse_labels[label] = currentIndex\n",
        "                currentIndex += 1\n",
        "            distinct_chars = add_word_distinct_chars(name, distinct_chars)\n",
        "            max_len = find_max_len(name, max_len)\n",
        "            temp_list = []\n",
        "            temp_list.append(name)\n",
        "            temp_list.append(label)\n",
        "            dataset.append(temp_list)\n",
        "    random.shuffle(dataset)\n",
        "    return dataset, distinct_chars, max_len, diverse_labels\n",
        "\n",
        "def assign_index(chars):\n",
        "    features = {}\n",
        "    for index, char in enumerate(chars):\n",
        "        features[char] = index\n",
        "    return features\n",
        "\n",
        "def convert_dataset_and_tokenize(dataset, features, max_len):\n",
        "    for item_no, example in enumerate(dataset):\n",
        "        name = example[0]\n",
        "        label = example[1]\n",
        "        tokenized = []\n",
        "        padding_needed = max_len - len(name)\n",
        "        for index in range(padding_needed):\n",
        "            tokenized.append(0)\n",
        "        for char in name:\n",
        "            token = features[char]\n",
        "            tokenized.append(token)\n",
        "        example[0] = tokenized\n",
        "        dataset[item_no] = example\n",
        "    return dataset\n",
        "\n",
        "def split_examples_labels(dataset):\n",
        "    examples = [entry[0] for entry in dataset]\n",
        "    labels = [entry[1] for entry in dataset]\n",
        "    return examples, labels\n",
        "\n",
        "def convert_labels_to_int(labels, diverse_labels):\n",
        "    for index, label in enumerate(labels):\n",
        "        labels[index] = diverse_labels[label]\n",
        "    return labels\n",
        "\n",
        "def build_model(max_features, max_len):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_features, 128, input_length = max_len))\n",
        "    model.add(LSTM(128))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(26, activation = 'softmax'))\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
        "    return model\n",
        "\n",
        "def return_fold(examples, labels_int, num_folds, current_fold):\n",
        "    interval = len(examples) // num_folds\n",
        "    current_start = (current_fold - 1) * interval\n",
        "    current_end = current_fold * interval\n",
        "    X_test = examples[current_start:current_end]\n",
        "    y_test = labels_int[current_start:current_end]\n",
        "    X_train = examples[:current_start]\n",
        "    y_train = labels_int[:current_start]\n",
        "    X_train.extend(examples[current_end:])\n",
        "    y_train.extend(labels_int[current_end:])\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "def make_predictions(model, X_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred = numpy.argmax(y_pred, axis = 1)\n",
        "    return y_pred\n",
        "\n",
        "def return_confusion_matrix(y_test, y_pred):\n",
        "    return confusion_matrix(y_test, y_pred)\n",
        "\n",
        "def return_classification_report(y_test, y_pred, diverse_names):\n",
        "    target_names = []\n",
        "    for item in diverse_names.keys():\n",
        "        target_names.append(item)\n",
        "    print(classification_report(y_test, y_pred, target_names = target_names))\n",
        "    return None\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    dataset, distinct_chars, max_len, diverse_labels = load_data(\"dga_domains_full.csv\")\n",
        "    dataset = dataset[0:100000]\n",
        "    max_features = len(distinct_chars) + 1\n",
        "    features = assign_index(distinct_chars)\n",
        "    dataset = convert_dataset_and_tokenize(dataset, features, max_len)\n",
        "    examples, labels = split_examples_labels(dataset)\n",
        "    labels_int = convert_labels_to_int(labels, diverse_labels)\n",
        "\n",
        "    acc_per_fold = []\n",
        "    loss_per_fold = []\n",
        "    number_of_folds = 5\n",
        "    for fold in range(1, number_of_folds + 1):\n",
        "        X_train, y_train, X_test, y_test = return_fold(examples, labels_int, number_of_folds, fold)\n",
        "        model = build_model(max_features, max_len)\n",
        "        print(\"Training for fold: \", fold)\n",
        "        history = model.fit(X_train, y_train, batch_size = 128, epochs = 5, verbose = 1)\n",
        "        scores = model.evaluate(X_test, y_test, verbose = 1)\n",
        "        print(f'Score for fold {fold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "        acc_per_fold.append(scores[1] * 100)\n",
        "        loss_per_fold.append(scores[0])\n",
        "        y_pred = make_predictions(model, X_test)\n",
        "        print(return_confusion_matrix(y_test, y_pred))\n",
        "        return_classification_report(y_test, y_pred, diverse_labels)\n",
        "\n",
        "    # == Provide average scores ==\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print('Score per fold')\n",
        "    for i in range(0, len(acc_per_fold)):\n",
        "        print('------------------------------------------------------------------------')\n",
        "        print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "        print('------------------------------------------------------------------------')\n",
        "        print('Average scores for all folds:')\n",
        "        print(f'> Accuracy: {numpy.mean(acc_per_fold)} (+- {numpy.std(acc_per_fold)})')\n",
        "        print(f'> Loss: {numpy.mean(loss_per_fold)}')\n",
        "        print('------------------------------------------------------------------------')\n"
      ],
      "metadata": {
        "id": "MJMvYAjMK2of",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04d08971-5721-4bfb-e4e9-a854c5208fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for fold:  1\n",
            "Epoch 1/5\n",
            "625/625 [==============================] - 70s 104ms/step - loss: 1.4877 - accuracy: 0.5992\n",
            "Epoch 2/5\n",
            "625/625 [==============================] - 65s 104ms/step - loss: 0.9667 - accuracy: 0.7069\n",
            "Epoch 3/5\n",
            "625/625 [==============================] - 66s 105ms/step - loss: 0.8159 - accuracy: 0.7468\n",
            "Epoch 4/5\n",
            "625/625 [==============================] - 65s 103ms/step - loss: 0.7241 - accuracy: 0.7722\n",
            "Epoch 5/5\n",
            "625/625 [==============================] - 65s 104ms/step - loss: 0.6715 - accuracy: 0.7857\n",
            "625/625 [==============================] - 14s 22ms/step - loss: 0.5905 - accuracy: 0.8073\n",
            "Score for fold 1: loss of 0.5904750823974609; accuracy of 80.73499798774719%\n",
            "[[ 223    0  171    0    1    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    2    0]\n",
            " [   0  401    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    1    0    0    0    0    0    8    0    1]\n",
            " [  36    6 9752    1    8    5    3   38   17    9   24    0    6    0\n",
            "    64    4    3    0    6    0   73   42    1    1    4    0]\n",
            " [   0    0    7  317    0   14    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0   16    0    0    0    1    0    0    0]\n",
            " [   1    0  164    0  189    1    0    0    0    0   25    0    0    0\n",
            "     0    0    0    0    0    0    6    0    0    0    0    0]\n",
            " [   3    0    5   12    0  365    0    0    0    0    0    0    0    0\n",
            "     0    0    0    2    7    0    0    0    2    0    0    0]\n",
            " [   0    0   27   18    0   21  129    0    0   33    1    0   38    9\n",
            "     0    3   25    3   51    0    0    6    0    0    0   29]\n",
            " [  20    0   54    0    0    0    0  356    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0  119    0    1    0    0    0  255    0    5    0    0    0\n",
            "     0    0    0    0    0    0    1    0    0    0    0    0]\n",
            " [   0    0   52    4    5    0    2    0    0  334    0    0    5    0\n",
            "     0    4    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0  146    0    0    0    0    0    3    0  245    0    0    0\n",
            "     0    1    0    0    0    0    9    1    0    0    0    0]\n",
            " [   0    0    5    0    0    0    1    0    0    0    0  350    0    0\n",
            "     0    1    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0  108    0    0    0   18    0    2   80    3    0   83    0\n",
            "     0    8   34    0    1    0    2   60    0    0    0    1]\n",
            " [   0    0   39   28    0   39    1    0    0   34    2    0   31   10\n",
            "     0    2   63    5   98    0    0    2    1    0    0   39]\n",
            " [   2    0  275    0    0    0    0    0    0    2    0    0    0    0\n",
            "   122    0    1    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0   76    0    1    0    2    0    0    3    1    0    2    0\n",
            "     0  308    3    0    0    0    1    9    0    0    0    0]\n",
            " [   0    0   27    0    0    0    0    0    0    4    0    2    1    0\n",
            "     0    1  358    0    2    0    0    0    0    0    0    0]\n",
            " [   1    0    5    0    0   26   29    1    0    0    0    0    0    0\n",
            "     0    0    1  295   48    0    0    0    0    0    0    5]\n",
            " [   0    0   33    0    3   25    0    0    0    1    0    0    2    0\n",
            "     0    1   65    4  240    0    0    0    0    0    0    0]\n",
            " [   0    0    2    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    3    0    0    0  418    0    0    0    0    0    0]\n",
            " [   0    0    8    0    0    0    0    0    1    2    0    0    0    0\n",
            "     0    0    0    0    0    0  373    0    0    0    0    0]\n",
            " [   0    0  181    0    0    0    0    0    4   16    1    0   64    0\n",
            "     0   10    0    0    0    0    0  109    0    0    0    0]\n",
            " [   0    0    6    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0  415    0    0    0]\n",
            " [   0    5    0    2    0    0    2    0    0    0    0    0    0    1\n",
            "     0    0    0    0    0    0    0    0    0  403    0    1]\n",
            " [   0    0  379    1    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    1    0    0    0   29    0]\n",
            " [   1    0   44   23    3   32    3    0    1   31    1    0   24   12\n",
            "     0    1   44    1   58    1    0   13    0    0    0   68]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        gozi       0.78      0.56      0.65       397\n",
            "     corebot       0.97      0.98      0.97       411\n",
            "       alexa       0.83      0.97      0.90     10103\n",
            "     ranbyus       0.78      0.89      0.83       355\n",
            "       symmi       0.90      0.49      0.63       386\n",
            "      emotet       0.69      0.92      0.79       396\n",
            "    dircrypt       0.68      0.33      0.44       393\n",
            "      matsnu       0.90      0.83      0.86       430\n",
            "       simda       0.90      0.67      0.77       381\n",
            "      fobber       0.61      0.82      0.70       406\n",
            "      pushdo       0.80      0.60      0.69       405\n",
            "      qadars       0.99      0.98      0.99       357\n",
            "      kraken       0.32      0.21      0.25       400\n",
            "      ramnit       0.31      0.03      0.05       394\n",
            "      nymaim       0.66      0.30      0.41       402\n",
            "      pykspa       0.89      0.76      0.82       406\n",
            "       tinba       0.60      0.91      0.72       395\n",
            "     murofet       0.95      0.72      0.82       411\n",
            "cryptolocker       0.46      0.64      0.53       374\n",
            "       ramdo       1.00      0.99      0.99       423\n",
            "     vawtrak       0.80      0.97      0.88       384\n",
            "   conficker       0.45      0.28      0.35       385\n",
            "    padcrypt       0.99      0.99      0.99       421\n",
            "      rovnix       0.98      0.97      0.98       414\n",
            "    suppobox       0.83      0.07      0.13       410\n",
            "      necurs       0.47      0.19      0.27       361\n",
            "\n",
            "    accuracy                           0.81     20000\n",
            "   macro avg       0.75      0.66      0.67     20000\n",
            "weighted avg       0.79      0.81      0.78     20000\n",
            "\n",
            "Training for fold:  2\n",
            "Epoch 1/5\n",
            "625/625 [==============================] - 68s 106ms/step - loss: 1.4807 - accuracy: 0.6026\n",
            "Epoch 2/5\n",
            "625/625 [==============================] - 67s 107ms/step - loss: 0.9441 - accuracy: 0.7105\n",
            "Epoch 3/5\n",
            "625/625 [==============================] - 66s 106ms/step - loss: 0.7969 - accuracy: 0.7516\n",
            "Epoch 4/5\n",
            "625/625 [==============================] - 65s 105ms/step - loss: 0.7136 - accuracy: 0.7743\n",
            "Epoch 5/5\n",
            "625/625 [==============================] - 66s 106ms/step - loss: 0.6609 - accuracy: 0.7883\n",
            "625/625 [==============================] - 14s 22ms/step - loss: 0.5893 - accuracy: 0.8098\n",
            "Score for fold 2: loss of 0.5892881155014038; accuracy of 80.98000288009644%\n",
            "[[ 255    0  127    1    7    0    0   16    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0  376    3    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    1    2    0    0    0    0   13    0    0]\n",
            " [  80    3 9589    4   24    4    6   81   19   24   61    0    7    0\n",
            "    41    5    4    2   16    0   10   42    0    0   10    2]\n",
            " [   1    0    7  396    1   17    0    0    0    0    0    0    0    0\n",
            "     0    0    1    0   20    1    0    0    0    0    0    0]\n",
            " [   3    0  126    0  249    0    0    0    0    0   38    0    0    0\n",
            "     0    1    0    0    0    0    3    0    0    0    0    0]\n",
            " [   0    0    7    2    0  369    0    0    0    0    0    0    0    0\n",
            "     0    0    0    3   11    0    0    0    1    0    0    0]\n",
            " [   0    0   19   21    1   18  127    0    1   64    2    0   27    4\n",
            "     0    4   25   12   50    0    0    5    0    0    0   23]\n",
            " [   6    0   17    0    0    0    0  356    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0   80    0    1    0    0    0  277    0   18    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0   44    1    5    0    0    0    0  372    0    0    6    0\n",
            "     0    3    0    0    0    0    2    2    0    0    0    0]\n",
            " [   0    0   88    0    0    0    0    0    0    0  288    0    0    0\n",
            "     0    2    0    0    0    0    7    0    0    0    0    0]\n",
            " [   0    0    9    0    0    0    0    0    0    0    0  387    1    0\n",
            "     0    4    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0   96    0    1    0    3    0    3  131    2    0   68    9\n",
            "     0    8   22    0    0    0    0   65    0    0    0    0]\n",
            " [   0    0   34   46    4   29    2    0    1   55    0    0   30   12\n",
            "     0    0   24    5   95    1    0    0    0    0    0   44]\n",
            " [   3    0  314    0    0    0    0    0    0    0    0    0    0    0\n",
            "    83    0    1    0    0    0    0    0    0    0    2    0]\n",
            " [   0    0   53    0    0    0    0    0    1    4    3    0    2    0\n",
            "     0  300    1    0    0    0    1    6    0    0    0    0]\n",
            " [   0    2   19    0    1    0    0    0    0   10    0    0    0    0\n",
            "     0    1  364    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    5    0    0   28   20    0    0    0    0    0    0    0\n",
            "     0    0    2  326   38    0    0    0    0    0    0    6]\n",
            " [   0    0   31    0    1    4    0    0    1    2    0    0    0    1\n",
            "     0    1  100   10  236    1    0    0    0    0    0    0]\n",
            " [   0    0    1    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    5    0    0    0  384    0    0    0    0    0    0]\n",
            " [   0    0   74    0    0    0    0    0    0    2    3    0    3    0\n",
            "     0    0    0    0    0    0  333    2    0    0    0    0]\n",
            " [   0    0  186    0    0    0    0    0    1   23    1    0   47    0\n",
            "     0    4    0    0    0    0    0  125    0    0    0    0]\n",
            " [   0    0    1    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0  382    0    0    0]\n",
            " [   0    0    1    0    0    0    1    0    0    0    0    0    0    2\n",
            "     0    0    0    0    0    0    0    0    0  389    0    2]\n",
            " [   3    0  308    0    0    0    0    1    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0   49    0]\n",
            " [   1    0   34   29    0   25    1    0    0   47    2    0   41   15\n",
            "     0    3   32    5   70    0    0   12    1    0    0  104]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        gozi       0.72      0.63      0.67       406\n",
            "     corebot       0.99      0.95      0.97       395\n",
            "       alexa       0.85      0.96      0.90     10034\n",
            "     ranbyus       0.79      0.89      0.84       444\n",
            "       symmi       0.84      0.59      0.70       420\n",
            "      emotet       0.75      0.94      0.83       393\n",
            "    dircrypt       0.79      0.32      0.45       403\n",
            "      matsnu       0.78      0.94      0.85       379\n",
            "       simda       0.91      0.74      0.81       376\n",
            "      fobber       0.51      0.86      0.64       435\n",
            "      pushdo       0.69      0.75      0.72       385\n",
            "      qadars       1.00      0.97      0.98       401\n",
            "      kraken       0.29      0.17      0.21       408\n",
            "      ramnit       0.28      0.03      0.06       382\n",
            "      nymaim       0.67      0.21      0.31       403\n",
            "      pykspa       0.88      0.81      0.84       371\n",
            "       tinba       0.63      0.92      0.75       397\n",
            "     murofet       0.90      0.77      0.83       425\n",
            "cryptolocker       0.44      0.61      0.51       388\n",
            "       ramdo       0.99      0.98      0.99       390\n",
            "     vawtrak       0.94      0.80      0.86       417\n",
            "   conficker       0.48      0.32      0.39       387\n",
            "    padcrypt       0.99      1.00      1.00       383\n",
            "      rovnix       0.97      0.98      0.98       395\n",
            "    suppobox       0.80      0.14      0.23       361\n",
            "      necurs       0.57      0.25      0.34       422\n",
            "\n",
            "    accuracy                           0.81     20000\n",
            "   macro avg       0.75      0.67      0.68     20000\n",
            "weighted avg       0.80      0.81      0.79     20000\n",
            "\n",
            "Training for fold:  3\n",
            "Epoch 1/5\n",
            "625/625 [==============================] - 67s 104ms/step - loss: 1.4801 - accuracy: 0.6018\n",
            "Epoch 2/5\n",
            "625/625 [==============================] - 66s 105ms/step - loss: 0.9638 - accuracy: 0.7070\n",
            "Epoch 3/5\n",
            "625/625 [==============================] - 65s 104ms/step - loss: 0.8175 - accuracy: 0.7453\n",
            "Epoch 4/5\n",
            "625/625 [==============================] - 66s 105ms/step - loss: 0.7315 - accuracy: 0.7696\n",
            "Epoch 5/5\n",
            "625/625 [==============================] - 65s 105ms/step - loss: 0.6728 - accuracy: 0.7860\n",
            "625/625 [==============================] - 14s 22ms/step - loss: 0.5864 - accuracy: 0.8109\n",
            "Score for fold 3: loss of 0.5863624215126038; accuracy of 81.08500242233276%\n",
            "[[ 226    0  146    0    3    0    0   14    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    5    1]\n",
            " [   0  379    0    0    0    0    0    0    0    0    0    2    0    0\n",
            "     0    0    0    0    0    0    0    0    0   13    0    1]\n",
            " [  46    5 9564    2   47    8    2   64   36   18   60    1   17    1\n",
            "    24   10    6    3    6    1   12   26    1    1   34    1]\n",
            " [   0    0    3  368    0   25    0    0    0    0    0    0    0    0\n",
            "     0    0    1    0   27    0    0    0    1    0    0    0]\n",
            " [   1    0   95    0  254    0    0    0    0    0   31    0    0    0\n",
            "     0    1    0    0    0    2    1    0    0    0    0    0]\n",
            " [   0    0    6    1    1  359    0    0    0    0    0    0    0    0\n",
            "     0    0    0    9   16    2    0    0    0    0    0    0]\n",
            " [   0    1   22   12    2   19  105    0    0   55    1    0   57   18\n",
            "     0    8   19   22   40    0    0    1    0    0    0   24]\n",
            " [   3    0   12    0    0    0    0  405    0    0    0    0    0    0\n",
            "     0    0    0    1    0    0    0    0    0    0    0    0]\n",
            " [   0    0   60    0    7    0    0    0  308    0   11    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0   29    0    5    1    2    0    0  340    0    0   16    0\n",
            "     0    2    0    0    0    0    1    0    0    0    0    0]\n",
            " [   0    0   94    0    0    0    0    0    1    0  311    0    0    0\n",
            "     0    3    0    0    0    0    6    0    0    0    0    0]\n",
            " [   0    0    5    0    0    0    0    0    0    0    0  376    0    0\n",
            "     0    2    1    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0   91    0    0    0    4    0    1  105    0    0  126    5\n",
            "     0   12   10    0    0    0    0   31    0    0    0    0]\n",
            " [   0    0   46   46    2   30    0    0    0   51    2    0   50   35\n",
            "     0    6   31    2   84    1    1    1    0    0    0   27]\n",
            " [   4    0  313    0    1    0    0    0    0    0    0    0    0    1\n",
            "    69    0    0    0    0    0    0    0    0    0    1    0]\n",
            " [   0    0   48    0    0    0    0    0    4    3    3    0    1    0\n",
            "     0  326    2    0    0    0    0    3    0    0    0    0]\n",
            " [   0    0   14    0    4    0    0    0    0   10    0    0    1    0\n",
            "     0    3  371    0    0    0    0    0    0    0    0    0]\n",
            " [   0    1    4    0    0   22    5    0    0    0    0    0    0    0\n",
            "     0    0    2  304   33    1    0    0    0    0    0   11]\n",
            " [   0    0   20    0    3    2    0    0    0    3    0    0    0    1\n",
            "     0    3  109   14  242    0    0    0    0    0    0    1]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    1    0    0\n",
            "     0    7    0    0    0  415    0    0    0    0    0    0]\n",
            " [   0    0   85    0    0    0    0    0    0    1    2    0    2    0\n",
            "     0    0    0    0    0    0  339    0    0    0    0    0]\n",
            " [   0    0  203    0    1    0    0    0    3    4    5    0  109    0\n",
            "     0    7    0    0    0    0    0   83    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0  389    0    0    0]\n",
            " [   0    0    0    0    0    0    2    0    0    0    0    0    0    1\n",
            "     0    0    0    0    0    0    0    0    0  394    0    0]\n",
            " [   1    0  323    1    1    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0   75    0]\n",
            " [   0    0   51   32    1   21    0    0    0   32    1    0   47   32\n",
            "     0    2   36    4   64    0    0    8    0    0    0   54]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        gozi       0.80      0.57      0.67       395\n",
            "     corebot       0.98      0.96      0.97       395\n",
            "       alexa       0.85      0.96      0.90      9996\n",
            "     ranbyus       0.80      0.87      0.83       425\n",
            "       symmi       0.77      0.66      0.71       385\n",
            "      emotet       0.74      0.91      0.81       394\n",
            "    dircrypt       0.88      0.26      0.40       406\n",
            "      matsnu       0.84      0.96      0.90       421\n",
            "       simda       0.87      0.80      0.83       386\n",
            "      fobber       0.55      0.86      0.67       396\n",
            "      pushdo       0.73      0.75      0.74       415\n",
            "      qadars       0.99      0.98      0.98       384\n",
            "      kraken       0.30      0.33      0.31       385\n",
            "      ramnit       0.37      0.08      0.14       415\n",
            "      nymaim       0.74      0.18      0.29       389\n",
            "      pykspa       0.83      0.84      0.83       390\n",
            "       tinba       0.63      0.92      0.75       403\n",
            "     murofet       0.85      0.79      0.82       383\n",
            "cryptolocker       0.47      0.61      0.53       398\n",
            "       ramdo       0.98      0.98      0.98       423\n",
            "     vawtrak       0.94      0.79      0.86       429\n",
            "   conficker       0.54      0.20      0.29       415\n",
            "    padcrypt       0.99      1.00      1.00       389\n",
            "      rovnix       0.97      0.99      0.98       397\n",
            "    suppobox       0.65      0.19      0.29       401\n",
            "      necurs       0.45      0.14      0.21       385\n",
            "\n",
            "    accuracy                           0.81     20000\n",
            "   macro avg       0.75      0.68      0.68     20000\n",
            "weighted avg       0.80      0.81      0.79     20000\n",
            "\n",
            "Training for fold:  4\n",
            "Epoch 1/5\n",
            "625/625 [==============================] - 67s 105ms/step - loss: 1.4885 - accuracy: 0.5986\n",
            "Epoch 2/5\n",
            "625/625 [==============================] - 66s 105ms/step - loss: 0.9689 - accuracy: 0.7042\n",
            "Epoch 3/5\n",
            "625/625 [==============================] - 65s 104ms/step - loss: 0.8131 - accuracy: 0.7486\n",
            "Epoch 4/5\n",
            "625/625 [==============================] - 65s 104ms/step - loss: 0.7216 - accuracy: 0.7736\n",
            "Epoch 5/5\n",
            "216/625 [=========>....................] - ETA: 42s - loss: 0.6796 - accuracy: 0.7867"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ο παραπάνω κώδικας επιλύει το πρόβλημα της ταξινόμησης ονομάτων DNS ως προς το αν αυτά τα ονόματα είναι καλόβουλα ή έχουν παραχθεί από Domain Generation Algorithms (DGA's). Συγκεκριμένα, ο κώδικας επιλύει ένα πρόβλημα multi-class classification, όπου τα ονόματα αντιστοιχίζονται είτε στην κατηγορία alexa, εάν είναι καλόβουλα ή στην αντίστοιχα κατηγορία malware στην οποία ανήκουν. Το μοντέλο που έχει χρησιμοποιηθεί για την ταξινόμηση των ονομάτων είναι το LSTM, ενώ για το κατάλληλο validation του μοντέλου έχει χρησιμοποιηθεί η μέθοδος K-fold cross validation με Κ = 5. Τα δεδομένα εκπαίδευσης και αξιολόγησης έχουν ληφθεί από <a href=\"https://github.com/chrmor/DGA_domains_dataset\">εδώ</a> και χρησιμοποιούνται τα πρώτα 100000 ονόματα του αρχείου \"dga_domains_full.csv\". \n",
        "\n",
        "Μελετώντας τον κώδικα και εκτελώντας τον, να απαντήσετε στις ακόλουθες ερωτήσεις:\n",
        "<ul>\n",
        "<li>Γιατί το LSTM είναι κατάλληλο μοντέλο για την επίλυση του συγκεκριμένου προβλήματος;</li>\n",
        "<li>Τι είναι η λίστα ονομάτων Alexa (<a href=\"https://en.wikipedia.org/wiki/Alexa_Internet\">info</a>), ονόματα της οποίας έχουν χρησιμοποιηθεί για την κατηγορία των καλόβουλων ονομάτων;</li>\n",
        "<li>Να διαλέξετε δύο οικογένειες malware από την κατηγορία dga, δηλαδή την κατηγορία με τα κακόβουλα ονόματα που αντιστοιχούν σε malware. Να αναζητήσετε τι προβλήματα δημιουργούν τα malware αυτά, π.χ. υποκλοπή τραπεζικών κωδικών, έναρξη επιθέσεων DDoS, κλπ.</li>\n",
        "<li>Να περιγράψετε σύντομα τα βήματα που ακολουθεί το παραπάνω πρόγραμμα για την επίλυση του προβλήματος.</li>\n",
        "<li>Ποιος είναι ο ρόλος του embedding layer και τι παραμέτρους δέχεται;</li>\n",
        "<li>Να αναφέρετε λόγους για τους οποίους χρησιμοποιείται η μέθοδος K-fold cross validation.</li>\n",
        "<li>Κατά τη χρήση της μεθόδου K-fold cross validation, ανά fold, να αναφέρετε πόσα examples χρησιμοποιούνται για την εκπαίδευση του μοντέλου και πόσα για το validation/testing.\n",
        "<li>Δείτε <a href=\"https://en.wikipedia.org/wiki/Cross-validation_(statistics)\">εδώ</a> μεθόδους που μπορεί να χρησιμοποιούν έναντι της μεθόδου K-fold cross validation. Να αναφέρετε μερικά πλεονεκτήματα και μειονεκτήματά τους.</li>\n",
        "<li>Να αναλύσετε τα κριτήρια precision, recall και F1-score που εμφανίζονται στο classification report.</li>\n",
        "<li>Σε ποιες κατηγορίες πετυχαίνουμε καλύτερα αποτελέσματα και σε ποιες χειρότερα; Γιατί πιστεύετε ότι παίρνουμε αυτά τα αποτελέσματα;</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "MZNG1h91eQwn"
      }
    }
  ]
}