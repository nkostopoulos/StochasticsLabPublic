{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stochastic_Processes_&_Optimization_in_Machine_Learning_(Lab_2_Autoencoders).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Άσκηση στους Autoencoders**\n",
        "\n",
        "Στα πλαίσια της άσκησης θα εξετάσετε πως δουλεύει ένας αυτόματος κωδικοποιητής (autoencoder) για την ταξινόμηση ενός συνόλου δεδομένων.\n",
        "\n"
      ],
      "metadata": {
        "id": "FJxOYmQGsJ9c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUfLRNadlqMU"
      },
      "outputs": [],
      "source": [
        "# train autoencoder for classification with no compression in the bottleneck layer\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from matplotlib import pyplot"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Θα χρησιμοποιήσουμε τη συνάρτηση make_classification() scikit-learn για να ορίσουμε ένα συνθετικό σύνολο δεδομένων ταξινόμησης (2 κλάσεων) με 100 χαρακτηριστικά εισόδου (στήλες) και 1.000 παραδείγματα (γραμμές). \n",
        "\n",
        "**Προσοχή**: *Στο δοθέν πρόβλημα τα περισσότερα από τα χαρακτηριστικά είναι περιττά κατά 90%, επιτρέποντας στον autoencoder αργότερα να μάθει μια πραγματικά χρήσιμη συμπιεσμένη αναπαράσταση.*\n"
      ],
      "metadata": {
        "id": "DxTYHY6MqXdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# synthetic classification dataset\n",
        "from sklearn.datasets import make_classification\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=100, n_informative=10, n_redundant=90, random_state=1)\n",
        "# number of input columns\n",
        "n_inputs = X.shape[1]\n",
        "# summarize the dataset\n",
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvZStKrylrW6",
        "outputId": "e4659e8e-a5ce-4d9d-9e52-290503bea6f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 100) (1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Στη συνέχεια, θα αναπτύξουμε ένα μοντέλο αυτόματου κωδικοποιητή [Multilayer Perceptron (MLP)](https://en.wikipedia.org/wiki/Multilayer_perceptron).\n",
        "\n",
        "Το μοντέλο θα λάβει όλες τα παραδείγματα στην είσοδο και μετά θα εξάγει τις ίδιες τιμές.Συνεπώς θα μάθει να αναδημιουργεί το ίδιο μοτίβο\n",
        "\n",
        "Ο αυτόματος κωδικοποιητής αποτελείται από δύο μέρη τον κωδικοποιητή και τον αποκωδικοποιητή.\n",
        "\n",
        "Μόλις εκπαιδευτεί ο autoencoder, ο αποκωδικοποιητής δεν θα χρησιμοποιηθεί ξανά και κρατάμε μόνο τον κωδικοποιητή για να συμπιέσουμε παραδείγματα εισόδου στα διανύσματα που εξάγονται από το bottleneck.\n",
        "\n",
        "Σε αυτόν τον πρώτο αυτόματο κωδικοποιητή, δεν θα συμπιέσουμε καθόλου την είσοδο και θα χρησιμοποιήσουμε ενδιάμεσες συνάψεις ίδιου αριθμού με τον αριθμό των κόμβων εισόδου (άρα ίσο με τον αριθμό των διαστάσεων του δείγματος). Αυτό θα εφαρμοστεί ώστε το μοντέλο να μάθει σχεδόν τέλεια και να επιβεβαιώσουμε ότι το μοντέλο έχει εφαρμοστεί σωστά.\n",
        "\n",
        "Πριν ορίσουμε και προσαρμόσουμε το μοντέλο, θα χωρίσουμε τα δεδομένα σε σύνολα εκπαίδευσης και δοκιμής και θα κλιμακώσουμε τα δεδομένα εισόδου κανονικοποιώντας τις τιμές στο εύρος 0-1."
      ],
      "metadata": {
        "id": "Do8y5OsirqST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split into train test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
        "# scale data\n",
        "t = MinMaxScaler()\n",
        "t.fit(X_train)\n",
        "X_train = t.transform(X_train)\n",
        "X_test = t.transform(X_test)"
      ],
      "metadata": {
        "id": "4q80eRAOlrg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Θα ορίσουμε ότι ο κωδικοποιητής θα έχει δύο κρυφά επίπεδα, το πρώτο με διπλάσιο αριθμό διαστάσεων εισόδου (π.χ. 200) και το δεύτερο με τον αρχικό αριθμό διαστάσεων (100).\n"
      ],
      "metadata": {
        "id": "bESsv4FTuDRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define encoder\n",
        "visible = Input(shape=(n_inputs,))\n",
        "# encoder level 1\n",
        "e = Dense(n_inputs*2)(visible)\n",
        "e = BatchNormalization()(e)\n",
        "e = LeakyReLU()(e)\n",
        "# encoder level 2\n",
        "e = Dense(n_inputs)(e)\n",
        "e = BatchNormalization()(e)\n",
        "e = LeakyReLU()(e)\n",
        "# bottleneck\n",
        "n_bottleneck = n_inputs\n",
        "bottleneck = Dense(n_bottleneck)(e)"
      ],
      "metadata": {
        "id": "bfg4IAu1lro8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Στη συνέχεια ορίζουμε τον αποκωδικοποιητή. Ο αποκωδικοποιητής θα έχει την αντίστροφη δομή από τον κωδικοποιητή."
      ],
      "metadata": {
        "id": "OYjQdojUuzZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define decoder, level 1\n",
        "d = Dense(n_inputs)(bottleneck)\n",
        "d = BatchNormalization()(d)\n",
        "d = LeakyReLU()(d)\n",
        "# decoder level 2\n",
        "d = Dense(n_inputs*2)(d)\n",
        "d = BatchNormalization()(d)\n",
        "d = LeakyReLU()(d)\n",
        "# output layer\n",
        "output = Dense(n_inputs, activation='linear')(d)\n",
        "# define autoencoder model\n",
        "model = Model(inputs=visible, outputs=output)"
      ],
      "metadata": {
        "id": "C0uZPgH4lrsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Στη συνέχεια κάνουμε compile το μοντέλο μας. Για να εντοπίσουμε την διαφορά μεταξύ του trained και του test, χρησιμοποιούμε την μέθοδο των ελάχιστων τετραγώνων."
      ],
      "metadata": {
        "id": "yYOGOrbava3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compile autoencoder model\n",
        "model.compile(optimizer='adam', loss='mse')"
      ],
      "metadata": {
        "id": "wAZytGAKlrvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Στη συνέχεια, μπορούμε να εκπαιδεύσουμε το μοντέλο να αναπαράγει τα δεδομένα εισόδου και να παρακολουθεί την απόδοση του μοντέλου σε σύγκριση με το test set.\n",
        "\n",
        "**Ερώτηση 1**: Ποια είναι η διαφορά μεταξύ mini-batch και epochs\n"
      ],
      "metadata": {
        "id": "WwYSO4-wvqAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the autoencoder model to reconstruct input\n",
        "history = model.fit(X_train, X_train, epochs=200, batch_size=16, verbose=2, validation_data=(X_test,X_test))"
      ],
      "metadata": {
        "id": "bdAfIRN5lr0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Παρακάτω παρατείθενται οι καμπύλες μάθησης για το train και το test dataset."
      ],
      "metadata": {
        "id": "dOCUfjzFwbsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot loss\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend(title = \"datasets\")\n",
        "pyplot.title(\"Loss\")\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "UvD6Bp3ulr3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ερώτηση 2**: Δοκιμάστε να τρέξετε το μοντέλο με Bottleneck: α) 50 και β) 25. Τι παρατηρείτε ως προς τα αποτελέσματα του μοντέλου;\n",
        "**Ερώτηση 3**: δοκιμάστε να βάλετε τρία επίπεδα (τα δύο υφιστάμενα και ένα ακόμα με bottleneck 50). Τι διαφορές παρατηρείτε;"
      ],
      "metadata": {
        "id": "cM6PAjvzym_j"
      }
    }
  ]
}