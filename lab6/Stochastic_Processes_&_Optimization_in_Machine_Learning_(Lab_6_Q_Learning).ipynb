{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Stochastic_Processes_&_Optimization_in_Machine_Learning_(Lab_6_Q_Learning).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPgkIB2VSC2TZi9WDXhivEs"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"_-E2yjf_xmvf","colab_type":"text"},"source":["<h1><b>Q-Learning</b></h1>\n","<p align=\"justify\">Στην συγκεκριμένη άσκηση θα μελετήσετε το στοχαστικό αλγόριθμο <i><a href=\"https://en.wikipedia.org/wiki/Q-learning\">Q-Learning</a></i>, χρησιμοποιώντας το έτοιμο πρόγραμμα που σας δίνεται. Με το συγκεκριμένο πρόγραμμα μπορείτε να βρείτε τη μικρότερη διαδρομή μεταξύ δύο σημείων όπως φαίνεται στο παρακάτω σχήμα (στο δοθέν παράδειγμα η αρχή είναι το σημείο 0 και ο τελικός προορισμός το σημείο 7):</p>\n","<img src=\"https://raw.githubusercontent.com/nkostopoulos/StochasticsLabPublic/master/lab6/ex6a.PNG\"></img>\n","<p align=\"justify\">Με βάση το τον κώδικα που σας έχει δοθεί, καλείστε να απαντήσετε στα παρακάτω ερωτήματα:</p>\n","<ul>\n","<li>Να περιγράψετε σύντομα τον αλγόριθμο <i>Q-Learning</i>. Σε ποια προβλήματα\n","θεωρείτε ότι ταιριάζει ως τρόπος εκμάθησης η <i><a href=\"https://en.wikipedia.org/wiki/Reinforcement_learning\">Ενισχυτική Μάθηση (Reinforcement Learning)</a></i>; Ποια είναι η βασική διαφορά του αλγορίθμου <i>Q-Learning</i> από τους αλγορίθμους <i>Policy Iteration</i> και <i>Value Iteration</i>;</li>\n","<li>Σχεδιάστε τη <i>μήτρα διασύνδεσης των κόμβων</i> (με -1 θα συμβολίζετε τις περιπτώσεις όπου δεν υπάρχει διασύνδεση μεταξύ των κόμβων, με 0 την περίπτωση όπου υπάρχει απλή διασύνδεση μεταξύ των κόμβων και με 100 όταν ένας κόμβος διασυνδέεται με τον τερματικό κόμβο).</li>\n","<li>Εκτελέστε το πρόγραμμα που σας έχει δοθεί και στη συνέχεια αλλάξτε τον τρόπο που συνδέονται οι κόμβοι. Τι παρατηρείτε ως προς τα αποτελέσματα;</li>\n","<li>Έστω ότι οι κόμβοι έχουν και πρόσθετη πληροφορία (εκτός από τον τρόπο διασύνδεσης τους) που ο agent θα μπορεί να την αξιοποιήσει για να φτάσει στον προορισμό του. Τι θα συνέβαινε ως προς τα αποτελέσματα;</li>\n","</ul>"]},{"cell_type":"code","metadata":{"id":"Soa-Q-qzxyMb","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pylab as plt\n","\n","# map cell to cell, add circular cell to goal point\n","points_list = [(0,1), (1,5), (5,6), (5,4), (1,2), (2,3), (2,7)]\n","\n","# set the goal\n","goal = 7\n","\n","# how many points in graph? x points\n","MATRIX_SIZE = 8\n","\n","# create matrix x*y\n","R = np.matrix(np.ones(shape=(MATRIX_SIZE, MATRIX_SIZE)))\n","R *= -1\n","\n","# assign zeros to paths and 100 to goal-reaching point\n","for point in points_list:\n","    print(point)\n","    if point[1] == goal:\n","        R[point] = 100\n","    else:\n","        R[point] = 0\n","\n","    if point[0] == goal:\n","        R[point[::-1]] = 100\n","    else:\n","        # reverse of point\n","        R[point[::-1]]= 0\n","\n","# add goal point round trip\n","R[goal,goal]= 100\n","\n","Q = np.matrix(np.zeros([MATRIX_SIZE,MATRIX_SIZE]))\n","\n","# learning parameter\n","gamma = 0.8\n","\n","initial_state = 1\n","\n","def available_actions(state):\n","    current_state_row = R[state,]\n","    av_act = np.where(current_state_row >= 0)[1]\n","    return av_act\n","\n","available_act = available_actions(initial_state) \n","\n","def sample_next_action(available_actions_range):\n","    next_action = int(np.random.choice(available_act,1))\n","    return next_action\n","\n","action = sample_next_action(available_act)\n","\n","def update(current_state, action, gamma):\n","    \n","  max_index = np.where(Q[action,] == np.max(Q[action,]))[1]\n","  \n","  if max_index.shape[0] > 1:\n","      max_index = int(np.random.choice(max_index, size = 1))\n","  else:\n","      max_index = int(max_index)\n","  max_value = Q[action, max_index]\n","  \n","  Q[current_state, action] = R[current_state, action] + gamma * max_value\n","  print('max_value', R[current_state, action] + gamma * max_value)\n","  \n","  if (np.max(Q) > 0):\n","    return(np.sum(Q/np.max(Q)*100))\n","  else:\n","    return (0)\n","    \n","update(initial_state, action, gamma)\n","\n","# Training\n","scores = []\n","for i in range(700):\n","    current_state = np.random.randint(0, int(Q.shape[0]))\n","    available_act = available_actions(current_state)\n","    action = sample_next_action(available_act)\n","    score = update(current_state,action,gamma)\n","    scores.append(score)\n","    print ('Score:', str(score))\n","    \n","print(\"Trained Q matrix:\")\n","print(Q/np.max(Q)*100)\n","\n","# Testing\n","current_state = 0\n","steps = [current_state]\n","\n","while current_state != 7:\n","\n","    next_step_index = np.where(Q[current_state,] == np.max(Q[current_state,]))[1]\n","    \n","    if next_step_index.shape[0] > 1:\n","        next_step_index = int(np.random.choice(next_step_index, size = 1))\n","    else:\n","        next_step_index = int(next_step_index)\n","    \n","    steps.append(next_step_index)\n","    current_state = next_step_index\n","\n","print(\"Most efficient path:\")\n","print(steps)\n","\n","plt.plot(scores)\n","plt.show()"],"execution_count":0,"outputs":[]}]}
